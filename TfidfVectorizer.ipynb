{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CjykNMT6TUX1"},"outputs":[],"source":["w = [1, 2, 3]\n","b = -10\n","z = "]},{"cell_type":"code","source":["import numpy as np\n","from collections import OrderedDict"],"metadata":{"id":"SUeKHJlWZwWw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Задание**\n"],"metadata":{"id":"36IDMhspabTh"}},{"cell_type":"code","source":["import numpy as np\n","\n","\n","class TfidfVectorizer:\n","\n","    def __init__(self, w2v_model):\n","        self.sorted_vocab = {}\n","        self.w2v_model = w2v_model\n","\n","    def fit1(self, X):\n","        return self\n","\n","    def fit(self, texts):\n","        self.n_texts = len(texts)\n","        vocabulary = OrderedDict()\n","        for text in texts:\n","            text = set(text.split())\n","            for word in text:\n","                vocabulary[word] = vocabulary.get(word, 0) + 1\n","        self.vocabulary = vocabulary\n","\n","    def _single_transform(self, text):\n","        text = text.split()\n","        count = {}\n","        for word in text:\n","            count[word] = count.get(word, 0) + 1\n","        output = []\n","\n","        for word in self.vocabulary:\n","            tf = count.get(word, 0) / len(text)\n","            n_texts_with_word = self.vocabulary.get(word, 0)\n","            if n_texts_with_word == 0:\n","                idf = 0\n","            else: \n","                idf = np.log(self.n_texts / n_texts_with_word)\n","            output.append(tf * idf)\n","        return output\n","\n","    def transform(self, X):\n","        \n","        X_transformed = np.zeros((len(X), self.w2v_model.wv.vector_size))\n","        for i, title in enumerate(X):\n","            \n","            title_vector = np.zeros((self.w2v_model.wv.vector_size,))\n","            tokens = self.re.findall(title.lower())\n","            for token in tokens:\n","                if token in self.w2v_model.wv.key_to_index:\n","                    title_vector += self.w2v_model.wv.get_vector(token)\n","                    \n","            X_transformed[i] = title_vector\n","        \n","        return X_transformed\n","\n","\n","def read_input():\n","    n1, n2 = map(int, input().split())\n","\n","    train_texts = [input().strip() for _ in range(n1)]\n","    test_texts = [input().strip() for _ in range(n2)]\n","\n","    return train_texts, test_texts \n","\n","def solution():\n","    train_texts, test_texts = read_input()\n","    vectorizer = TfidfVectorizer()\n","    vectorizer.fit(train_texts)\n","    transformed = vectorizer.transform(test_texts)\n","\n","    for row in transformed:\n","        row_str = ' '.join(map(str, np.round(row, 3)))\n","        print(row_str)\n","\n","solution()"],"metadata":{"id":"U52-oXtyac3_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**решение со степика, проходит только 1 тест**\n"],"metadata":{"id":"Djzle8mc91Oq"}},{"cell_type":"code","source":["class TfidfVectorizer:\n","\n","    def __init__(self):\n","        self.sorted_vocab = {}\n","\n","\n","    def fit(self, texts):\n","        self.n_texts = len(texts)\n","        vocabulary = OrderedDict()\n","        for text in texts:\n","            text = set(text.split())\n","            for word in text:\n","                vocabulary[word] = vocabulary.get(word, 0) + 1\n","       # self.vocabulary = sorted(vocabulary)\n","        self.vocabulary = {key: value for key, value in sorted(vocabulary.items())}\n","        #print(vocabulary)\n","        \n","    def _single_transform(self, text):\n","        text = text.split()\n","        count = {}\n","        for word in text:\n","            count[word] = count.get(word, 0) + 1\n","        output = []\n","\n","        for word in self.vocabulary:\n","            tf = count.get(word, 0) / len(text)\n","            n_texts_with_word = self.vocabulary.get(word, 0)\n","            if n_texts_with_word == 0:\n","                idf = 0\n","            else: \n","                idf = np.log(self.n_texts / n_texts_with_word)\n","            output.append(tf * idf)\n","            #print(\"word =\", word, \" n_texts_with_word =\", n_texts_with_word, \" len(text) =\", len(text), \" count.get(word, 0) =\", count.get(word, 0), ' tf =', tf, 'idf =', idf, \"tf*idf =\", tf*idf)\n","        return output\n","    \n","    def transform(self, texts):\n","        output = []\n","        for text in texts:\n","            ans = self._single_transform(text)\n","            output.append(ans)\n","        return output\n","\n","\n","    def transform1(self, X):\n","        \n","        X_transformed = np.zeros((len(X), self.w2v_model.wv.vector_size))\n","        for i, title in enumerate(X):\n","            \n","            title_vector = np.zeros((self.w2v_model.wv.vector_size,))\n","            tokens = self.re.findall(title.lower())\n","            for token in tokens:\n","                if token in self.w2v_model.wv.key_to_index:\n","                    title_vector += self.w2v_model.wv.get_vector(token)\n","                    \n","            X_transformed[i] = title_vector\n","        \n","        return X_transformed\n","\n","\n","def read_input():\n","    #n1, n2 = 3, 2 #map(int, input().split())\n","\n","    #train_texts = ['a1 a1 a1', 'a1 b1', 'c1'] #[input().strip() for _ in range(n1)]\n","    #test_texts = ['a1 c1', 'd1'] #[input().strip() for _ in range(n2)]\n","\n","    n1, n2 = map(int, input().split())\n","    train_texts = [input().strip() for _ in range(n1)]\n","\n","    test_texts = [input().strip() for _ in range(n2)]\n","\n","    return train_texts, test_texts \n","\n","def solution():\n","    train_texts, test_texts = read_input()\n","    vectorizer = TfidfVectorizer()\n","    vectorizer.fit(train_texts)\n","    transformed = vectorizer.transform(test_texts)\n","\n","    for row in transformed:\n","        row_str = ' '.join(map(str, np.round(row, 3)))\n","        print(row_str)\n","\n","solution()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PLPDdZPi95ee","executionInfo":{"status":"ok","timestamp":1667944563411,"user_tz":-180,"elapsed":10874,"user":{"displayName":"Business True","userId":"15990550246125825368"}},"outputId":"6087609a-b411-4d8e-c8f1-ec53c32d7dc9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3 2\n","a a a\n","a b\n","c \n","a c\n","d\n","0.203 0.0 0.549\n","0.0 0.0 0.0\n"]}]},{"cell_type":"markdown","source":["**Мое решение (кусочек)**"],"metadata":{"id":"eoOx0Aitada_"}},{"cell_type":"code","source":["def read_input():\n","    n1, n2 = map(int, input().split())\n","\n","    train_texts = [input().strip() for _ in range(n1)]\n","    test_texts = [input().strip() for _ in range(n2)]\n","\n","    return train_texts, test_texts \n","\n","def solution():\n","   # train_texts, test_texts = read_input()\n","    train_texts = ['a a a', 'a b', 'c']\n","    #print(set(' '.join(train_texts).split()))\n","    TF_TDF(train_texts)\n","    vectorizer = TfidfVectorizer()\n","    vectorizer.fit(train_texts)\n","   # transformed = vectorizer.transform(test_texts)\n","\n","def TF_TDF(X):\n","  X_str = ' '.join(X)\n","  D = len(X)\n","\n","  set_words = list(set(' '.join(X).split()))\n","  for i in range(len(set_words)):\n","    print(set_words[i], X_str.count(set_words[i]) )\n","  \n","  dictionary = {set_words[i]: [0]*len(X)  for i in range(len(set_words))} #X_str.count(set_words[i])\n","  dictionary_IDF = {set_words[i]: []  for i in range(len(set_words))} #X_str.count(set_words[i])\n","  for key in dictionary_IDF:\n","    for i in range(len(X)):\n","      if X[i].count(key) > 0:\n","        dictionary_IDF[key] += [i]\n","\n","  print(dictionary_IDF)\n","  print(dictionary)\n","\n","  for sentence in X:\n","    sentence = sentence.split()\n","    for word in sentence:\n","      \n","      nt = sentence.count(word)\n","      Nt = len(sentence)\n","      print(word)\n","      d = len(dictionary_IDF[word])\n","      TF_IDF = (nt/Nt)*np.log(D/d)\n","      print('nt =', nt, ' Nt =', Nt,  ' d =', d, ' D =', D)\n","      print(TF_IDF, '\\n') \n","\n","solution()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":825},"id":"7lSKDeTEZaAe","executionInfo":{"status":"error","timestamp":1667762798872,"user_tz":-180,"elapsed":523,"user":{"displayName":"Алексей Кириллов","userId":"08391371030049465087"}},"outputId":"f834f01d-656a-4a43-d3f6-6ca09c9225da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["a 4\n","c 1\n","b 1\n","{'a': [0, 1], 'c': [2], 'b': [1]}\n","{'a': [0, 0, 0], 'c': [0, 0, 0], 'b': [0, 0, 0]}\n","a\n","nt = 3  Nt = 3  d = 2  D = 3\n","0.4054651081081644 \n","\n","a\n","nt = 3  Nt = 3  d = 2  D = 3\n","0.4054651081081644 \n","\n","a\n","nt = 3  Nt = 3  d = 2  D = 3\n","0.4054651081081644 \n","\n","a\n","nt = 1  Nt = 2  d = 2  D = 3\n","0.2027325540540822 \n","\n","b\n","nt = 1  Nt = 2  d = 1  D = 3\n","0.5493061443340549 \n","\n","c\n","nt = 1  Nt = 1  d = 1  D = 3\n","1.0986122886681098 \n","\n"]},{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-6f7d042eb822>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTF_IDF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0msolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-3-6f7d042eb822>\u001b[0m in \u001b[0;36msolution\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m#print(set(' '.join(train_texts).split()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mTF_TDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m    \u001b[0;31m# transformed = vectorizer.transform(test_texts)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"PLGhtG76b5Wt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"9UP4jzePb5TT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"lqTDxwAZb5QO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nY371HYsb5NS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","corpus = ['']\n","vectorizer = TfidfVectorizer()\n","X = vectorizer.fit_transform(corpus)\n","#vectorizer.get_feature_names_out()\n","print(X)\n"],"metadata":{"id":"Wju0bMeaZoBM","colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"status":"error","timestamp":1667912141266,"user_tz":-180,"elapsed":21,"user":{"displayName":"Business True","userId":"15990550246125825368"}},"outputId":"921bc89b-25b2-45f4-ab13-aa8661f64791"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ce2bc93f22e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m#vectorizer.get_feature_names_out()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   2075\u001b[0m         \"\"\"\n\u001b[1;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2077\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2079\u001b[0m         \u001b[0;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1328\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1330\u001b[0;31m         \u001b[0mvocabulary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1332\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1219\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1220\u001b[0m                 raise ValueError(\n\u001b[0;32m-> 1221\u001b[0;31m                     \u001b[0;34m\"empty vocabulary; perhaps the documents only contain stop words\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1222\u001b[0m                 )\n\u001b[1;32m   1223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"]}]},{"cell_type":"code","source":["vectorizer.get_feature_names_out()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cAoSuxddbBzE","executionInfo":{"status":"ok","timestamp":1667763100597,"user_tz":-180,"elapsed":257,"user":{"displayName":"Business True","userId":"15990550246125825368"}},"outputId":"7b16e4ad-00fa-4bb9-c85e-182a81182c51"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['arrow', 'bowl', 'currency'], dtype=object)"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["from collections import OrderedDict\n","import numpy as np\n","\n","class MyTdIdf:\n","    def fit(self, texts):\n","        self.n_texts = len(texts)\n","        vocabulary = OrderedDict()\n","        for text in texts:\n","            text = set(text.split())\n","            for word in text:\n","                vocabulary[word] = vocabulary.get(word, 0) + 1\n","        self.vocabulary = vocabulary\n","\n","    def _single_transform(self, text):\n","        text = text.split()\n","        count = {}\n","        for word in text:\n","            count[word] = count.get(word, 0) + 1\n","        output = []\n","\n","        for word in self.vocabulary:\n","            tf = count.get(word, 0) / len(text)\n","            n_texts_with_word = self.vocabulary.get(word, 0)\n","            if n_texts_with_word == 0:\n","                idf = 0\n","            else: \n","                idf = np.log(self.n_texts / n_texts_with_word)\n","            output.append(tf * idf)\n","        return output\n","\n","    def transform(self, texts):\n","        output = []\n","        for text in texts:\n","            ans = self._single_transform(text)\n","            output.append(ans)\n","        return output\n","\n","tfidf = MyTdIdf()\n","tfidf.fit([\n","    'a a a',\n","    'a b',\n","    'c'\n","])\n","\n","out = tfidf.transform([\n","    'a c',\n","    'd'\n","])\n","print(out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BcUHH4qMbVg6","executionInfo":{"status":"ok","timestamp":1667765100762,"user_tz":-180,"elapsed":257,"user":{"displayName":"Business True","userId":"15990550246125825368"}},"outputId":"c9e1fc1c-a564-424b-c033-b5bebf790753"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.2027325540540822, 0.0, 0.5493061443340549], [0.0, 0.0, 0.0]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"3uMFQVPjh2Lh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from collections import OrderedDict\n","\n","class TfidfVectorizer:\n","\n","    def __init__(self):\n","        self.sorted_vocab = {}\n","\n","\n","    def fit(self, texts):\n","        self.n_texts = len(texts)\n","        vocabulary = OrderedDict()\n","        for text in texts:\n","            text = set(text.split())\n","            for word in text:\n","                vocabulary[word] = vocabulary.get(word, 0) + 1\n","       # self.vocabulary = sorted(vocabulary)\n","        self.vocabulary = {key: value for key, value in sorted(vocabulary.items())}\n","        #print(vocabulary)\n","        \n","    def _single_transform(self, text):\n","        text = text.split()\n","        count = {}\n","        for word in text:\n","            count[word] = count.get(word, 0) + 1\n","        output = []\n","\n","        for word in self.vocabulary:\n","            tf = count.get(word, 0) / len(text)\n","            n_texts_with_word = self.vocabulary.get(word, 0)\n","            if n_texts_with_word == 0:\n","                idf = 0\n","            else: \n","                idf = np.log(self.n_texts / n_texts_with_word)\n","            output.append(tf * idf)\n","            #print(\"word =\", word, \" n_texts_with_word =\", n_texts_with_word, \" len(text) =\", len(text), \" count.get(word, 0) =\", count.get(word, 0), ' tf =', tf, 'idf =', idf, \"tf*idf =\", tf*idf)\n","        return output\n","    \n","    def transform(self, texts):\n","        output = []\n","        for text in texts:\n","            ans = self._single_transform(text)\n","            output.append(ans)\n","        return output\n","\n","\n","    def transform1(self, X):\n","        \n","        X_transformed = np.zeros((len(X), self.w2v_model.wv.vector_size))\n","        for i, title in enumerate(X):\n","            \n","            title_vector = np.zeros((self.w2v_model.wv.vector_size,))\n","            tokens = self.re.findall(title.lower())\n","            for token in tokens:\n","                if token in self.w2v_model.wv.key_to_index:\n","                    title_vector += self.w2v_model.wv.get_vector(token)\n","                    \n","            X_transformed[i] = title_vector\n","        \n","        return X_transformed\n","\n","\n","def read_input():\n","    #n1, n2 = 3, 2 #map(int, input().split())\n","\n","    #train_texts = ['a1 a1 a1', 'a1 b1', 'c1'] #[input().strip() for _ in range(n1)]\n","    #test_texts = ['a1 c1', 'd1'] #[input().strip() for _ in range(n2)]\n","\n","    n1, n2 = map(int, input().split())\n","    train_texts = [input().strip() for _ in range(n1)]\n","\n","    test_texts = [input().strip() for _ in range(n2)]\n","\n","    return train_texts, test_texts \n","\n","def solution():\n","    train_texts, test_texts = read_input()\n","    vectorizer = TfidfVectorizer()\n","    vectorizer.fit(train_texts)\n","    transformed = vectorizer.transform(test_texts)\n","\n","    for row in transformed:\n","        row_str = ' '.join(map(str, np.round(row, 3)))\n","        print(row_str)\n","\n","solution()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"apkptGvVOt1S","executionInfo":{"status":"ok","timestamp":1667996896992,"user_tz":-180,"elapsed":63249,"user":{"displayName":"Business True","userId":"15990550246125825368"}},"outputId":"70463027-d761-4242-82fd-408db2cf180a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5 3\n","aboba boba mokd\n","aboba aboba \n","z dbf\n","dbf dbf\n","z boba aboba\n","a boba aboba\n","mokd dbf\n","dbf z z z\n","0.17 0.305 0.0 0.0 0.0\n","0.0 0.0 0.458 0.805 0.0\n","0.0 0.0 0.229 0.0 0.687\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"9E6-nekTOtuI"},"execution_count":null,"outputs":[]}]}