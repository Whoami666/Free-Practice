{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/Ww/lfn1Dr5iySZFYau0Y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zcbjG_khHDdq","executionInfo":{"status":"ok","timestamp":1667944087639,"user_tz":-180,"elapsed":8668,"user":{"displayName":"Business True","userId":"15990550246125825368"}},"outputId":"8a4e3277-600b-4406-eb89-ddf2963e9916"},"outputs":[{"output_type":"stream","name":"stdout","text":["3 2\n","a a a\n","a b\n","c\n","a c\n","d\n","{'00': 0.6438410362258904, '02': 0.8465735902799727}\n","00\n","02\n"]}],"source":["import numpy as np\n","import math\n","\n","class TfidfVectorizer:\n","\n","    def __init__(self):\n","        self.sorted_vocab = {}\n","\n","    def IDF(self, corpus, all_set):\n","        idf_dict = {}    \n","        total_docs = len(corpus) \n","        \n","        for word in all_set: \n","            cnt = 0\n","            for row in corpus:\n","                split_row = row.split()\n","                count_word = split_row.count(word)\n","                if count_word > 0:\n","                  cnt += count_word\n","            idf_dict[word] = 1 + math.log((1+total_docs)/(1+cnt)) \n","        return idf_dict\n","\n","    def fit(self, X):\n","      \n","        all_set = ''\n","        for sentence in X:\n","          all_set += sentence\n","          all_set += ' '\n","        all_set = sorted(list(set(all_set.strip().split())))\n","        vocab = {j:i for i,j in enumerate(all_set)}\n","        idfs_ = self.IDF(X, all_set)\n","        self.features, self.idfs =  fit(corpus)\n","        return self\n","\n","\n","    def transform(self, dataset, features, idfs):\n","        ans_dict = {}\n","        for row in range(0, len(dataset)):\n","            word_count = Counter(dataset[row].split(' '))\n","            for word in dataset[row].split(' '):\n","                if word in list(features.keys()):\n","                    tf = word_count[word] / len(dataset[row].split(' '))\n","                    tfidf = tf * idfs_[word]\n","                    ans_dict[str(row) + str(features[word])] = tfidf\n","        print(ans_dict)\n","        return ans_dict\n","\n","    #tfidf_corpus = transform(corpus)\n","\n","def read_input():\n","    n1, n2 = map(int, input().split())\n","\n","    train_texts = [input().strip() for _ in range(n1)]\n","    test_texts = [input().strip() for _ in range(n2)]\n","\n","    return train_texts, test_texts \n","\n","def solution():\n","    train_texts, test_texts = read_input()\n","    vectorizer = TfidfVectorizer()\n","    vectorizer.fit(train_texts)\n","    transformed = vectorizer.transform(test_texts, vectorizer.features, vectorizer.idfs)\n","\n","    for row in transformed:\n","        print(row)\n","        \n","\n","solution()"]},{"cell_type":"code","source":["from collections import Counter\n","from tqdm import tqdm\n","\n","import math\n","import operator\n","import numpy\n","corpus = ['a a a', 'a b', 'c']\n","\n","def IDF(corpus, unique_words):\n","    idf_vals = {}    \n","    total_docs = len(corpus) \n","    for word in unique_words: \n","        cnt = 0\n","        for row in corpus:\n","            if word in row.split(\" \"): \n","                cnt+=1 \n","        idf_vals[word] = 1 + math.log((1+total_docs)/(1+cnt)) \n","    return idf_vals\n","\n","\n","def fit(dataset):    \n","    unique_words = set() \n","    if isinstance(dataset, (list,)):\n","        for row in dataset:\n","            for word in row.split(\" \"):\n","                unique_words.add(word)\n","        unique_words = sorted(list(unique_words))\n","        vocab = {j:i for i,j in enumerate(unique_words)}\n","    idfs_ = IDF(dataset, unique_words)\n","    return vocab, idfs_\n","features, idfs_ = fit(corpus)\n","\n","\n","def transform(dataset):\n","    sparse_matrix = csr_matrix((len(dataset), len(features)), dtype=float)\n","    for row in range(0, len(dataset)):\n","        word_count = Counter(dataset[row].split(' '))\n","        for word in dataset[row].split(' '):\n","            if word in list(features.keys()):\n","                tf = word_count[word] / len(dataset[row].split(' '))\n","                tfidf = tf * idfs_[word]\n","                sparse_matrix[row, features[word]] = tfidf\n","    output = normalize(sparse_matrix, norm='l2', axis = 1, copy=True, return_norm=False)\n","    return output\n","tfidf_corpus = transform(corpus)\n","\n","idf_list = list(idfs_.values())\n","print(\"Our idfs: \", idf_list) #This will give us idf vals calc by our custom implementation\n","print(tfidf_corpus[0].toarray())  #This will give us tfidf vals for the first document calc by our custom implementation"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":371},"id":"GKXDMfPZJM-Z","executionInfo":{"status":"error","timestamp":1667943014098,"user_tz":-180,"elapsed":18,"user":{"displayName":"Business True","userId":"15990550246125825368"}},"outputId":"e72e0233-e650-44d5-fd21-d7126b45e042"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-743f02aba1b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msparse_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'l2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mtfidf_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0midf_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midfs_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-743f02aba1b0>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0msparse_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsr_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mword_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'csr_matrix' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"EnGHfShoJV5I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from collections import OrderedDict\n","import numpy as np\n","\n","class MyTdIdf:\n","    def fit(self, texts):\n","        self.n_texts = len(texts)\n","        vocabulary = OrderedDict()\n","        for text in texts:\n","            text = set(text.split())\n","            for word in text:\n","                vocabulary[word] = vocabulary.get(word, 0) + 1\n","        self.vocabulary = vocabulary\n","\n","    def _single_transform(self, text):\n","        text = text.split()\n","        count = {}\n","        for word in text:\n","            count[word] = count.get(word, 0) + 1\n","        output = []\n","\n","        for word in self.vocabulary:\n","            tf = count.get(word, 0) / len(text)\n","            n_texts_with_word = self.vocabulary.get(word, 0)\n","            if n_texts_with_word == 0:\n","                idf = 0\n","            else: \n","                idf = np.log(self.n_texts / n_texts_with_word)\n","            output.append(tf * idf)\n","        return output\n","\n","    def transform(self, texts):\n","        output = []\n","        for text in texts:\n","            ans = self._single_transform(text)\n","            output.append(ans)\n","        return output\n","\n","tfidf = MyTdIdf()\n","tfidf.fit([\n","    'a a a a',\n","    'a b b',\n","    'c'\n","])\n","\n","out = tfidf.transform([\n","    'a c',\n","    'd a'\n","])\n","print(out)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2rFyMIFlN2HN","executionInfo":{"status":"ok","timestamp":1667944131797,"user_tz":-180,"elapsed":16,"user":{"displayName":"Business True","userId":"15990550246125825368"}},"outputId":"7003fbb8-15b0-47ae-9fe7-6c48dcbbf241"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[0.2027325540540822, 0.0, 0.5493061443340549], [0.2027325540540822, 0.0, 0.0]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Fcrb6YsrN2Al"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Вам нужно реализовать метод .forward() CBOW.\n","\n","В первой строчке на вход программе подаются два числа: vocab\\_sizevocab_size (размер словаря) и embedding\\_dimensionembedding_dimension (размер word embeddings). Следующая строка задает входной вектор индексов контекстных слов.\n","\n","Для решения инициализирована матрица embeddingsembeddings размера vocab\\_size \\times embedding\\_dimensionvocab_size×embedding_dimension и матрица contextscontexts размера embedding\\_dimension \\times vocab\\_sizeembedding_dimension×vocab_size, используйте их в решении. В качестве функции агрегации векторов контекста используется сумма.\n","\n","В ответе ожидается вектор с вероятностями размера 1 \\times vocab\\_size1×vocab_size.**"],"metadata":{"id":"y3urkHPGq-fv"}},{"cell_type":"code","source":["import numpy as np\n","import torch.nn as nn\n","import torch\n","\n","class CBOW(nn.Module):\n","\n","    def __init__(self, vocab_size: int, embedding_dim: int, random_state: int = 1):\n","        super().__init__()\n","        np.random.seed(random_state)\n","        self.vocab_size = vocab_size\n","        self.embedding_dim = embedding_dim\n","        self.embeddings = self.init_weight_matrix()\n","        self.contexts = self.init_weight_matrix().T\n","        self.linear = nn.Linear(\n","            in_features=embedding_dim,\n","            out_features=vocab_size,)\n","\n","    def init_weight_matrix(self, ):\n","        W = np.random.uniform(size=(self.vocab_size, self.embedding_dim))\n","        return W\n","\n","    def forward(self, x):              \n","          x = self.emb(x)\n","          x = self.linear(x)\n","          x = self.contexts (x)                \n","          return x        \n","\n","\n","\n","    def forward1(self, x):\n","        x = self.embeddings(x)\n","        x = self.linear(x)\n","        return x\n","\n","\n","\n","    def forward(self, x): \n","      print(\"embedding_dim\", self.embedding_dim)\n","      print(\"embeddings\", self.embeddings)\n","      x = torch.tensor(self.embeddings)\n","      x = self.linear(x)\n","     # x = torch.mean(x, dim=1)\n","      x = self.contexts(x) \n","      return x\n","\n","\n","\n","def read_vector(dtype=int):\n","    return np.array(list(map(dtype, input().split())))\n","\n","def solution():\n","    vocab_size, embedding_dim = read_vector()\n","    input_vector = read_vector()\n","\n","    cbow = CBOW(vocab_size, embedding_dim)\n","    output = cbow.forward(input_vector).round(3)\n","    print(' '.join(map(str, output)))\n","\n","\n","solution()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":505},"id":"wWQEr3HcrBRt","executionInfo":{"status":"error","timestamp":1668008096582,"user_tz":-180,"elapsed":5510,"user":{"displayName":"Business True","userId":"15990550246125825368"}},"outputId":"41b6596d-8832-4854-d936-45b6c96ffd5a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["5 2\n","4 0 3\n","embedding_dim 2\n","embeddings [[4.17022005e-01 7.20324493e-01]\n"," [1.14374817e-04 3.02332573e-01]\n"," [1.46755891e-01 9.23385948e-02]\n"," [1.86260211e-01 3.45560727e-01]\n"," [3.96767474e-01 5.38816734e-01]]\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-2fa33355d87d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0msolution\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-2fa33355d87d>\u001b[0m in \u001b[0;36msolution\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0mcbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCBOW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcbow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-19-2fa33355d87d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     39\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"embeddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m      \u001b[0;31m# x = torch.mean(x, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontexts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7v4oatrUr50W"},"execution_count":null,"outputs":[]}]}